## 腾讯优图 视觉算法岗

### 一面（技术面）

#### 1. 介绍LSTM和GRU的动机，原理和方法
答： 两者都是为了解决 RNN 中梯度消失严重，不能捕捉长程依赖的问题。注意讲清楚LSTM和GRU是为什么可以解决梯度消失的

#### 2. Batch Norm是什么？为什么需要它？以及Batch Norm是如何实现的？
答： 解决深度网络中Internal Covariate Shift，数据分布的变化，有利于加快网络收敛。实现方式：基于batch的统计采样，两个变量：average_mean, average std

#### 3. 卷积的循环实现方式？介绍caffe中im2col的实现方式。
答： 循环，滑动窗口； 待补充

#### 4. 1x1卷积的作用
答： 通道融合; 配合非线性激活，可以以较少的参数，增加网络的非线性; 降维/升维(主要是减少/增加feature map的channel数量)

#### 5. 对resnet/残差的理解
答： 直接拟合目标的难度要大于拟合残差和恒等映射的难度; shortcut 一定程度上解决了深度网络中梯度无法有效回传到浅层的问题

#### 6. 介绍下FPN
答： 待补充

---

## 阿里巴巴 图像算法岗

### 一面（技术面）

*时隔太久，记不得都问了些啥。其中有几个问题印象比较深刻。由于我项目中用到了SegNet，所以问了几个由SegNet延伸开的问题。*

#### 1. 阐述一下SegNet的网络结构，比如有几层，每层的层数和channel数。

*由于SegNet是我自己复现的，面试前又草草复习了一下，所以记得比较清楚。*

答： VGG-16去掉最后面三个fc，作为前半部分的降采样网络。自底向上，分为五种size的conv，每个conv对应的层数分别为2、2、3、3、3，输出的channel数分别为64、128、256、512、512。同个conv中，每个卷积层的输出channel数相同。并设计了完全相对称的另外13层作为后半部分的上采样。一共26层。卷积操作均采用简单的3×3滑窗，通过小卷积核的逐层叠加，达到和大卷积核相同规模的感受野，同时节省参数量，这部分在VGGNet论文里有具体阐述；同时，stride=1，padding='SAME'，保证了每个conv中的每层输出都能维持相同的size；另外，每个conv后跟一个2×2的pooling，故而每次池化后size都缩小到原来的1/2 × 1/2。

#### 2. 为什么不用FCN？

答： 试过之后发现分割效果不行，还是SegNet效果比较好。

#### 3. 为什么FCN分割效果不行？

*因为对FCN不熟，所以回答的时候心里很没底。*

答： 我觉得是因为FCN尾端只通过一个简单的反卷积，就来实现上采样，所以导致回归出来的结果图不够精细吧。比如轮廓会出现严重的锯齿状。

#### 4. 那SegNet中有哪些连接的设计？

*感觉是面试官埋的一个坑，故意把话题往错误的地方引，看你能不能及时发现并跳出来。如果你很坚定自己的判断，那么估计会加分。*

答： （一开始没听懂所谓的“连接”指什么，后来再三确认，原来面试官说的是跨层的连接结构）是那种类似identity mapping的跨层连接？没有吧，这是FCN中才有的设计，用于融合底层丰富的细节信息和顶层丰富的语义信息。SegNet我亲自复现过，很确定在SegNet中没有此类的设计。

### 二面（技术面）

1. 介绍简历上的项目，就几处细节发问。比如项目中如果遇到了XX问题，而又不可以通过XX等方法绕过，那么你该如何解决它。
2. 深度学习的以两个问题。
3. 在线平台，手撸代码。不难，估计考察的是代码熟练程度（能不能快速敲出来）和代码风格（敲得漂不漂亮）。
4. 介绍自己的github贡献，并就几处地方发问。

### 三面（技术面）

面的最糟的一轮。
面试官问了很多非常尖锐的问题，一副“简历打假者”的姿态。很多地方我做项目的时候都没注意到，被问得后背直凉。
比如，面试官问“XX项目中，甲方为什么提出那样的验收要求？他们以XX来作为验收要求不是更好吗”；“你这么处理，是不是不符合常理？”；“为什么basemodel要换成Xception，而不是ShuffleNet？”；“验收完后，他们拿回去会具体怎么落地？”；等等。
数度语塞，使尽平生的吹牛逼功力来包装做过的东西。讲话还要小心翼翼，生怕不能自圆其说，或者提到某些自己不熟悉的东西，被面试官追问。

### 四面（boss面）

问数构和算法。

### 五面（跨部门boss面）

就简历发问项目细节。还问了几个深度学习和图像相关的问题，其中问到了调参经验。

### 六面（HR面）

聊天，小伙子有没有对象啊，巴拉巴拉。

---

